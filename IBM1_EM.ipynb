{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM 1 and 2, Expectation Maximization\n",
    "\n",
    "f is the source language, e the target language. In this code, we use the convention that we represent our data in the following way:\n",
    "(source, target, alignment)\n",
    "\n",
    "### 1. First we collect all imports, read in the data and initialize the system's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from aer import read_naacl_alignments, AERSufficientStatistics\n",
    "\n",
    "\n",
    "import random\n",
    "import codecs\n",
    "import math\n",
    "import tqdm\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "# Number of iterations\n",
    "S = 10\n",
    "\n",
    "# Set paths for data\n",
    "training_english_path = \"training/hansards.36.2.e\"\n",
    "training_french_path = \"training/hansards.36.2.f\"\n",
    "validation_english_path = \"validation/dev.e\"\n",
    "validation_french_path = \"validation/dev.f\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open(training_english_path, 'r', 'utf8') as f:\n",
    "    training_english = [line.split() for line in f.readlines()]\n",
    "\n",
    "with codecs.open(training_french_path, 'r', 'utf8') as f:\n",
    "    training_french = [line.split() for line in f.readlines()]\n",
    "\n",
    "training_data = list(zip(training_french, training_english))\n",
    "\n",
    "# Add NULL characters at the start of english sentences\n",
    "for i, (f, e) in enumerate(training_data):\n",
    "    e = [\"NULL\"] + e\n",
    "    training_data[i] = (f, e)\n",
    "\n",
    "with codecs.open(validation_english_path, 'r', 'utf8') as f:\n",
    "    validation_english = [line.split() for line in f.readlines()]\n",
    "\n",
    "with codecs.open(validation_french_path, 'r', 'utf8') as f:\n",
    "    validation_french = [line.split() for line in f.readlines()]\n",
    "\n",
    "validation_data = list(zip(validation_french, validation_english))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The IBM1 model EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IBM1_EM:\n",
    "    def __init__(self, training_data, validation_data, valid_align_path, training_iterations):\n",
    "        self.data = training_data\n",
    "        self.validation_data = validation_data\n",
    "        self.training_iterations = training_iterations\n",
    "        self.valid_align_path = valid_align_path\n",
    "        self.t = self.init_t()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train IBM1 by expectation maximization\"\"\"\n",
    "        for s in range(0, self.training_iterations):\n",
    "            ll = self.log_likelihood()\n",
    "            print(\"Log likelihood: {}\".format(ll))\n",
    "            self.test()\n",
    "            print(\"Iteration {}\".format(s + 1))\n",
    "            c_1 = Counter()\n",
    "            c_2 = Counter()\n",
    "            n = len(self.data)\n",
    "\n",
    "            for k in tqdm.tqdm(range(n)):\n",
    "                # extract all info for the current sentence \n",
    "                pair = self.data[k]\n",
    "                e_sentence = pair[1]\n",
    "                f_sentence = pair[0]\n",
    "\n",
    "                # loop over all positions in both sentences\n",
    "                for f in f_sentence:\n",
    "                    sentence_prob = sum([self.t[(f, e2)] for e2 in e_sentence])\n",
    "                    for e in e_sentence:\n",
    "                        delta = self.t[(f, e)] / sentence_prob\n",
    "                        # update the counts\n",
    "                        c_1[(e, f)] += delta\n",
    "                        c_2[e] += delta\n",
    "\n",
    "            # after looping over the counts, re-estimate t and q\n",
    "            self.update_t(c_1, c_2)\n",
    "\n",
    "    def init_t(self):\n",
    "        \"\"\"Initialize the transition probabilities randomly. This is a counter object.\"\"\"\n",
    "        vocabulary = defaultdict(list)\n",
    "        for f, e in self.data:\n",
    "            for w1 in f:\n",
    "                for w2 in e:\n",
    "                    vocabulary[w2].append(w1)\n",
    "        t = Counter()\n",
    "        for e in vocabulary:\n",
    "            words = list(set(vocabulary[e]))\n",
    "            probs = np.array([1 for i in range(len(words))])\n",
    "            probs = probs / sum(probs)\n",
    "            for i, f in enumerate(words):\n",
    "                t[(f, e)] = probs[i] \n",
    "        return t\n",
    "\n",
    "    def update_t(self, c_1, c_2):\n",
    "        \"\"\"Update the transition probabilities.\n",
    "\n",
    "        Args:\n",
    "            c_1: counts for english and french words occurring together\n",
    "            c_2: counts for english words on their own\n",
    "\n",
    "        Returns:\n",
    "            Counter object\n",
    "        \"\"\"\n",
    "        for all_f, all_e in self.data:\n",
    "            for f in all_f:\n",
    "                for e in all_e:\n",
    "                    self.t[(f, e)] = c_1[(e, f)] / c_2[e]\n",
    "\n",
    "    def log_likelihood(self):\n",
    "        \"\"\"Calculate log likelihood of IBM1 model.\n",
    "\n",
    "        Args:\n",
    "            data: list of aligned sentences in tuples (french, english)\n",
    "            t: Counter object, transition probabilities\n",
    "\n",
    "        Returns:\n",
    "            float\n",
    "        \"\"\"\n",
    "        log_likelihood = 0\n",
    "        for all_f, all_e in self.data:\n",
    "            likelihood = 1\n",
    "            # Sum over all alignments using ibm1 trick\n",
    "            for f in all_f:\n",
    "                probs = []\n",
    "                for e in all_e:\n",
    "                    probs.append(self.t[(f, e)])\n",
    "                likelihood *= sum(probs)\n",
    "            likelihood = ((1 / float(1 + len(all_e)))**(len(all_f))) * likelihood\n",
    "            if likelihood != 0:\n",
    "                log_likelihood += math.log(likelihood)\n",
    "        return log_likelihood\n",
    "\n",
    "    def align(self, f_sentence, e_sentence):\n",
    "        alignment = []\n",
    "        for i, f in enumerate(f_sentence):\n",
    "            alignment_i = None\n",
    "            best_score = -1\n",
    "            for j, e in enumerate(e_sentence):\n",
    "                score = self.t[(f, e)]\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    alignment_i = j\n",
    "            alignment.append((alignment_i + 1, i + 1))\n",
    "        return alignment\n",
    "\n",
    "    def test(self):\n",
    "        from random import random\n",
    "        # 1. Read in gold alignments\n",
    "        gold_sets = read_naacl_alignments(self.valid_align_path)\n",
    "\n",
    "        # 2. Here you would have the predictions of your own algorithm, \n",
    "        #  for the sake of the illustration, I will cheat and make some predictions by corrupting 50% of sure gold alignments\n",
    "        predictions = []\n",
    "        for i, (f, e) in enumerate(self.validation_data):\n",
    "            links = set(self.align(f, e))\n",
    "            predictions.append(links)\n",
    "\n",
    "        # 3. Compute AER\n",
    "\n",
    "        # first we get an object that manages sufficient statistics \n",
    "        metric = AERSufficientStatistics()\n",
    "        # then we iterate over the corpus \n",
    "        for gold, pred in zip(gold_sets, predictions):\n",
    "            metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "        # AER\n",
    "        print(metric.aer())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train our IBM1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -35225718.83664254\n",
      "0.8309726156751652\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                               | 0/231164 [00:00<?, ?it/s]\n",
      "  0%|                                     | 88/231164 [00:00<04:30, 854.40it/s]\n",
      "  0%|                                    | 137/231164 [00:00<05:49, 661.84it/s]\n",
      "  0%|                                    | 173/231164 [00:00<06:51, 561.69it/s]\n",
      "  0%|                                    | 207/231164 [00:00<07:39, 502.43it/s]\n",
      "  0%|                                    | 241/231164 [00:00<08:40, 443.61it/s]\n",
      "  0%|                                    | 279/231164 [00:00<08:54, 431.89it/s]\n",
      "  0%|                                    | 312/231164 [00:00<09:26, 407.31it/s]\n",
      "  0%|                                    | 390/231164 [00:00<08:34, 448.28it/s]\n",
      "  0%|                                    | 443/231164 [00:00<08:28, 453.89it/s]\n",
      "  0%|                                    | 489/231164 [00:01<08:29, 452.62it/s]\n",
      "  0%|                                    | 535/231164 [00:01<09:09, 419.61it/s]\n",
      "  0%|                                    | 588/231164 [00:01<09:02, 424.99it/s]\n",
      "  0%|                                    | 632/231164 [00:01<09:02, 424.99it/s]\n",
      "  0%|                                    | 689/231164 [00:01<08:50, 434.15it/s]\n",
      "  0%|                                    | 756/231164 [00:01<08:33, 448.40it/s]\n",
      "  0%|▏                                  | 1017/231164 [00:01<06:44, 568.64it/s]\n",
      "  0%|▏                                  | 1131/231164 [00:01<06:37, 579.11it/s]\n",
      "  1%|▏                                  | 1232/231164 [00:02<06:30, 588.35it/s]\n",
      "  1%|▏                                  | 1324/231164 [00:02<06:32, 585.06it/s]\n",
      "  1%|▏                                  | 1405/231164 [00:02<06:43, 569.29it/s]\n",
      "  1%|▏                                  | 1474/231164 [00:02<06:43, 569.11it/s]\n",
      "  1%|▏                                  | 1539/231164 [00:02<06:43, 568.95it/s]\n",
      "  1%|▏                                  | 1603/231164 [00:02<06:42, 570.67it/s]\n",
      "  1%|▎                                  | 1666/231164 [00:02<06:50, 559.62it/s]\n",
      "  1%|▎                                  | 1721/231164 [00:03<07:05, 539.33it/s]\n",
      "  1%|▎                                  | 1767/231164 [00:03<07:08, 535.59it/s]\n",
      "  1%|▎                                  | 1812/231164 [00:03<07:19, 521.59it/s]\n",
      "  1%|▎                                  | 1852/231164 [00:03<07:26, 513.48it/s]\n",
      "  1%|▎                                  | 1889/231164 [00:03<07:33, 505.49it/s]\n",
      "  1%|▎                                  | 1924/231164 [00:03<07:39, 498.83it/s]\n",
      "  1%|▎                                  | 1957/231164 [00:03<07:45, 492.58it/s]\n",
      "  1%|▎                                  | 1989/231164 [00:04<07:58, 479.28it/s]\n",
      "  1%|▎                                  | 2017/231164 [00:04<08:09, 468.42it/s]\n",
      "  1%|▎                                  | 2042/231164 [00:04<08:14, 463.14it/s]\n",
      "  1%|▎                                  | 2067/231164 [00:04<08:20, 458.11it/s]\n",
      "  1%|▎                                  | 2092/231164 [00:04<08:49, 432.32it/s]\n",
      "  1%|▎                                  | 2149/231164 [00:04<08:46, 434.58it/s]\n",
      "  1%|▎                                  | 2179/231164 [00:05<08:50, 431.48it/s]\n",
      "  1%|▎                                  | 2236/231164 [00:05<08:47, 434.01it/s]\n",
      "  1%|▎                                  | 2274/231164 [00:05<08:51, 430.44it/s]\n",
      "  1%|▎                                  | 2316/231164 [00:05<08:52, 430.08it/s]\n",
      "  1%|▎                                  | 2368/231164 [00:05<08:50, 431.64it/s]\n",
      "  1%|▎                                  | 2412/231164 [00:05<08:49, 431.64it/s]\n",
      "100%|█████████████████████████████████| 231164/231164 [08:11<00:00, 470.77it/s]\n"
     ]
    }
   ],
   "source": [
    "model = IBM1_EM(training_data, validation_data, 'validation/dev.wa.nonullalign', 10)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('prière', 'prayers'), 1.0),\n",
      " (('*', '*'), 0.90384884150794809),\n",
      " (('ensemble', 'Together'), 0.5),\n",
      " (('PLÉNIERS', 'WHOLE'), 0.5),\n",
      " (('travailler', 'Together'), 0.5),\n",
      " (('COMITÉS', 'WHOLE'), 0.5),\n",
      " (('le', 'Infrastructure'), 0.34144480421226403),\n",
      " (('YORK', 'WEST'), 0.33333333333333337),\n",
      " (('YORK', 'YORK'), 0.33333333333333337),\n",
      " (('-', 'WEST'), 0.33333333333333337),\n",
      " (('OUEST', 'YORK'), 0.33333333333333337),\n",
      " (('-', 'YORK'), 0.33333333333333337),\n",
      " (('OUEST', 'WEST'), 0.33333333333333337),\n",
      " (('chambre', 'COMMONS'), 0.33333333333333331),\n",
      " (('-', 'ROYAL'), 0.33333333333333331),\n",
      " (('COMMUNES', 'COMMONS'), 0.33333333333333331),\n",
      " (('ROYAL', 'ROYAL'), 0.33333333333333331),\n",
      " (('chambre', 'house'), 0.33333333333333331),\n",
      " (('mont', 'Royal'), 0.33333333333333331),\n",
      " (('table', 'contents'), 0.33333333333333331),\n",
      " (('MATIÈRES', 'contents'), 0.33333333333333331),\n",
      " (('DES', 'contents'), 0.33333333333333331),\n",
      " (('DES', 'house'), 0.33333333333333331),\n",
      " (('SIÈGE', 'vacancies'), 0.33333333333333331),\n",
      " (('DE', 'vacancies'), 0.33333333333333331)]\n"
     ]
    }
   ],
   "source": [
    "# Check whether the data makes sense\n",
    "pprint.pprint(model.t.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5561850802644004\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
