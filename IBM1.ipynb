{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM 1 and 2, Variational Bayes\n",
    "\n",
    "f is the source language, e the target language. In this code, we use the convention that we represent our data in the following way:\n",
    "(source, target)\n",
    "\n",
    "### 1. First we collect all imports, read in the data and initialize the system's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from aer import read_naacl_alignments, AERSufficientStatistics\n",
    "from scipy.special import digamma, loggamma\n",
    "\n",
    "import random\n",
    "import codecs\n",
    "import math\n",
    "import tqdm\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "# Number of iterations\n",
    "S = 10\n",
    "\n",
    "# Set paths for data\n",
    "training_english_path = \"training/hansards.36.2.e\"\n",
    "training_french_path = \"training/hansards.36.2.f\"\n",
    "validation_english_path = \"validation/dev.e\"\n",
    "validation_french_path = \"validation/dev.f\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open(training_english_path, 'r', 'utf8') as f:\n",
    "    training_english = [line.split() for line in f.readlines()]\n",
    "\n",
    "with codecs.open(training_french_path, 'r', 'utf8') as f:\n",
    "    training_french = [line.split() for line in f.readlines()]\n",
    "\n",
    "training_data = list(zip(training_french, training_english))\n",
    "\n",
    "# Add NULL characters at the start of english sentences\n",
    "for i, (f, e) in enumerate(training_data):\n",
    "    e = [\"NULL\"] + e\n",
    "    training_data[i] = (f, e)\n",
    "\n",
    "with codecs.open(validation_english_path, 'r', 'utf8') as f:\n",
    "    validation_english = [line.split() for line in f.readlines()]\n",
    "\n",
    "with codecs.open(validation_french_path, 'r', 'utf8') as f:\n",
    "    validation_french = [line.split() for line in f.readlines()]\n",
    "\n",
    "validation_data = list(zip(validation_french, validation_english))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The IBM1 model EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IBM1:\n",
    "    def __init__(self, training_data, validation_data, valid_align_path, training_iterations, training, alpha=-1):\n",
    "        self.data = training_data\n",
    "        self.validation_data = validation_data\n",
    "        self.training_iterations = training_iterations\n",
    "        self.valid_align_path = valid_align_path\n",
    "        self.t = self.init_t()\n",
    "        if training == \"VB\" and alpha == -1:\n",
    "            print(\"Please reinitialise with an alpha value >= 0.\")\n",
    "        elif training == \"VB\":\n",
    "            print(\"Initialised Variational Bayes model.\")\n",
    "        else:\n",
    "            print(\"Initialised Expectation Maximisation model.\")\n",
    "        self.training = training\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train IBM1 by expectation maximization\"\"\"\n",
    "        for s in range(0, self.training_iterations):\n",
    "            ll = self.log_likelihood()\n",
    "            print(\"Log likelihood: {}\".format(ll))\n",
    "            self.test()\n",
    "            print(\"Iteration {}\".format(s + 1))\n",
    "            c_1 = defaultdict(Counter)\n",
    "            c_2 = Counter()\n",
    "            n = len(self.data)\n",
    "\n",
    "            for k in tqdm.tqdm(range(n)):\n",
    "                # extract all info for the current sentence \n",
    "                pair = self.data[k]\n",
    "                e_sentence = pair[1]\n",
    "                f_sentence = pair[0]\n",
    "\n",
    "                # loop over all positions in both sentences\n",
    "                for f in f_sentence:\n",
    "                    sentence_prob = sum([self.t[e2][f] for e2 in e_sentence])\n",
    "                    for e in e_sentence:\n",
    "                        delta = self.t[e][f] / sentence_prob\n",
    "                        # update the counts\n",
    "                        c_1[e][f] += delta\n",
    "                        c_2[e] += delta\n",
    "\n",
    "            # after looping over the counts, re-estimate t and q\n",
    "            self.update_t(c_1, c_2)\n",
    "\n",
    "    def init_t(self):\n",
    "        \"\"\"Initialize the transition probabilities randomly. This is a counter object.\"\"\"\n",
    "        vocabulary = defaultdict(list)\n",
    "        for f, e in self.data:\n",
    "            for w1 in f:\n",
    "                for w2 in e:\n",
    "                    vocabulary[w2].append(w1)\n",
    "        t = defaultdict(Counter)\n",
    "        for e in vocabulary:\n",
    "            words = list(set(vocabulary[e]))\n",
    "            probs = np.array([1 for i in range(len(words))])\n",
    "            probs = probs / sum(probs)\n",
    "            for i, f in enumerate(words):\n",
    "                t[e][f] = probs[i] \n",
    "        return t\n",
    "\n",
    "    def update_t(self, c_1, c_2):\n",
    "        \"\"\"Update the transition probabilities.\n",
    "\n",
    "        Args:\n",
    "            c_1: counts for english and french words occurring together\n",
    "            c_2: counts for english words on their own\n",
    "\n",
    "        Returns:\n",
    "            Counter object\n",
    "        \"\"\"\n",
    "        if self.training == \"VB\" and self.alpha != -1:\n",
    "            for all_f, all_e in self.data:\n",
    "                for f in all_f:\n",
    "                    for e in all_e:\n",
    "                        self.t[e][f] = math.exp(digamma(c_1[e][f] + self.alpha) - digamma(c_2[e] + len(c_1[e])*self.alpha))\n",
    "        else:\n",
    "            for all_f, all_e in self.data:\n",
    "                for f in all_f:\n",
    "                    for e in all_e:\n",
    "                        self.t[(f, e)] = c_1[e][f] / c_2[e]\n",
    "\n",
    "    def log_likelihood(self):\n",
    "        \"\"\"Calculate log likelihood of IBM1 model.\n",
    "\n",
    "        Args:\n",
    "            data: list of aligned sentences in tuples (french, english)\n",
    "            t: Counter object, transition probabilities\n",
    "\n",
    "        Returns:\n",
    "            float\n",
    "        \"\"\"\n",
    "        log_likelihood = 0\n",
    "        for all_f, all_e in self.data:\n",
    "            likelihood = 1\n",
    "            # Sum over all alignments using ibm1 trick\n",
    "            for f in all_f:\n",
    "                probs = []\n",
    "                for e in all_e:\n",
    "                    probs.append(self.t[e][f])\n",
    "                likelihood *= sum(probs)\n",
    "            likelihood = ((1 / float(1 + len(all_e)))**(len(all_f))) * likelihood\n",
    "            if likelihood != 0 and not math.isnan(likelihood):\n",
    "                log_likelihood += math.log(likelihood)\n",
    "        return log_likelihood\n",
    "\n",
    "    def align(self, f_sentence, e_sentence):\n",
    "        alignment = []\n",
    "        for i, f in enumerate(f_sentence):\n",
    "            alignment_i = None\n",
    "            best_score = -1\n",
    "            for j, e in enumerate(e_sentence):\n",
    "                score = self.t[e][f]\n",
    "                if score >= best_score:\n",
    "                    best_score = score\n",
    "                    alignment_i = j\n",
    "            alignment.append((alignment_i + 1, i + 1))\n",
    "        return alignment\n",
    "\n",
    "    def test(self):\n",
    "        from random import random\n",
    "        # 1. Read in gold alignments\n",
    "        gold_sets = read_naacl_alignments(self.valid_align_path)\n",
    "\n",
    "        # 2. Here you would have the predictions of your own algorithm, \n",
    "        #  for the sake of the illustration, I will cheat and make some predictions by corrupting 50% of sure gold alignments\n",
    "        predictions = []\n",
    "        for i, (f, e) in enumerate(self.validation_data):\n",
    "            links = set(self.align(f, e))\n",
    "            predictions.append(links)\n",
    "\n",
    "        # 3. Compute AER\n",
    "\n",
    "        # first we get an object that manages sufficient statistics \n",
    "        metric = AERSufficientStatistics()\n",
    "        # then we iterate over the corpus \n",
    "        for gold, pred in zip(gold_sets, predictions):\n",
    "            metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "        # AER\n",
    "        print(metric.aer())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train our IBM1 model with EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_em = IBM1(training_data, validation_data, 'validation/dev.wa.nonullalign', 10, \"EM\")\n",
    "model_em.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the IBM1 model with Variational Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vb = IBM1(training_data, validation_data, 'validation/dev.wa.nonullalign', 10, \"VB\", 0.05)\n",
    "model_vb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check whether the data makes sense\n",
    "pprint.pprint(model.t.most_common(25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
